{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Parameter Tuning\n",
    "Training an optimal logistic regression model on the pruned dataset, exploring different solvers and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ExtractedFinalDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_features = []\n",
    "for i in range(8):\n",
    "    langevin = str(i) + \"__max_langevin_fixed_point__m_3__r_30\"\n",
    "    bad_features.append(langevin)\n",
    "    for j in range(9):\n",
    "        quantile = (j+1)*0.1\n",
    "        if quantile != 0.5:\n",
    "            feature_name = str(i) + \"__index_mass_quantile__q_\" + str(quantile)\n",
    "            bad_features.append(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(bad_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.index = df['9']\n",
    "df = df.drop(['9'], axis=1)\n",
    "df['Label'] = \"One\"\n",
    "df['Label'][2001.0 <= df.index ] = \"Two\"\n",
    "df['Label'][4001.0 <= df.index ] = \"Three\"\n",
    "df['Label'][6001.0 <= df.index ] = \"Four\"\n",
    "df['Label'][8001.0 <= df.index ] = \"Five\"\n",
    "df['Label'][10001.0 <= df.index ] = \"Six\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.map(lambda t: str(t))\n",
    "df = df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_features = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599, 1705)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsample = extracted_features.sample(frac=0.05).reset_index(drop=True)\n",
    "subsample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = subsample.drop(['Label'], 1)\n",
    "y = subsample['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test some of the different solvers and see how they perform comparatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LBFGS Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.754176610979\n",
      "Test accuracy: 0.572222222222\n",
      "{'C': 0.01, 'class_weight': None}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "             'class_weight': ['balanced', None],\n",
    "             }\n",
    "model = GridSearchCV(LogisticRegression(solver='lbfgs'), param_grid)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print 'Training accuracy:', model.score(np.array(X_train),np.array(y_train))\n",
    "print 'Test accuracy:', model.score(np.array(X_test), np.array(y_test))\n",
    "print model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sag Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.603818615752\n",
      "Test accuracy: 0.522222222222\n",
      "{'C': 0.01, 'class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "             'class_weight': ['balanced', None],\n",
    "             }\n",
    "model = GridSearchCV(LogisticRegression(solver='sag'), param_grid)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print 'Training accuracy:', model.score(np.array(X_train),np.array(y_train))\n",
    "print 'Test accuracy:', model.score(np.array(X_test), np.array(y_test))\n",
    "print model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton-CG Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.949880668258\n",
      "Test accuracy: 0.666666666667\n",
      "{'C': 0.1, 'class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "             'class_weight': ['balanced', None],\n",
    "             }\n",
    "model = GridSearchCV(LogisticRegression(solver='newton-cg'), param_grid)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print 'Training accuracy:', model.score(np.array(X_train),np.array(y_train))\n",
    "print 'Test accuracy:', model.score(np.array(X_test), np.array(y_test))\n",
    "print model.best_params_awful_features = []\n",
    "for i in range(8):\n",
    "    langevin = str(i) + \"__max_langevin_fixed_point__m_3__r_30\"\n",
    "    awful_features.append(langevin)\n",
    "    for j in range(9):\n",
    "        quantile = (j+1)*0.1\n",
    "        if quantile != 0.5:\n",
    "            feature_name = str(i) + \"__index_mass_quantile__q_\" + str(quantile)\n",
    "            awful_features.append(feature_name)\n",
    "sample = sample.drop(awful_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Solver (liblinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.856801909308\n",
      "Test accuracy: 0.616666666667\n",
      "{'C': 1, 'class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "             'class_weight': ['balanced', None],\n",
    "             }\n",
    "model = GridSearchCV(LogisticRegression(), param_grid)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print 'Training accuracy:', model.score(np.array(X_train),np.array(y_train))\n",
    "print 'Test accuracy:', model.score(np.array(X_test), np.array(y_test))\n",
    "print model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.627777777778\n",
      "{'C': 10, 'class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "             'class_weight': ['balanced', None],\n",
    "             }\n",
    "model = GridSearchCV(LogisticRegression(penalty='l1'), param_grid)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print 'Training accuracy:', model.score(np.array(X_train),np.array(y_train))\n",
    "print 'Test accuracy:', model.score(np.array(X_test), np.array(y_test))\n",
    "print model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as though newton-cg and the default liblinear have the best potential performance. Try more grid searching on their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.99522673031\n",
      "Test accuracy: 0.622222222222\n",
      "{'multi_class': 'multinomial', 'C': 10, 'class_weight': None}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 1, 10],\n",
    "             'class_weight': ['balanced', None],\n",
    "              'multi_class' : ['ovr', 'multinomial']\n",
    "             }\n",
    "model = GridSearchCV(LogisticRegression(solver='newton-cg', max_iter=1000), param_grid)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print 'Training accuracy:', model.score(np.array(X_train),np.array(y_train))\n",
    "print 'Test accuracy:', model.score(np.array(X_test), np.array(y_test))\n",
    "print model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try training a model on a 70-30 split of the full dataset, and see how it performs. This might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fullset = extracted_features.sample(frac=1).reset_index(drop=True)\n",
    "X = fullset.drop(['Label'], 1)\n",
    "y = fullset['Label']\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.795113230036\n",
      "Test accuracy: 0.793103448276\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='newton-cg', C=1, class_weight='balanced')\n",
    "model.fit(np.array(X_train_full),np.array(y_train_full))\n",
    "\n",
    "print 'Training accuracy:', model.score(np.array(X_train_full),np.array(y_train_full))\n",
    "print 'Test accuracy:', model.score(np.array(X_test_full), np.array(y_test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.779499404052\n",
      "Test accuracy: 0.778364849833\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='newton-cg', C=10, multi_class='multinomial')\n",
    "model.fit(np.array(X_train_full),np.array(y_train_full))\n",
    "\n",
    "print 'Training accuracy:', model.score(np.array(X_train_full),np.array(y_train_full))\n",
    "print 'Test accuracy:', model.score(np.array(X_test_full), np.array(y_test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.764123957092\n",
      "Test accuracy: 0.761401557286\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=1, class_weight='balanced')\n",
    "model.fit(np.array(X_train_full),np.array(y_train_full))\n",
    "\n",
    "print 'Training accuracy:', model.score(np.array(X_train_full),np.array(y_train_full))\n",
    "print 'Test accuracy:', model.score(np.array(X_test_full), np.array(y_test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.835637663886\n",
      "Test accuracy: 0.812013348165\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty='l1', C=0.001)\n",
    "model.fit(X_train_full, y_train_full)\n",
    "\n",
    "print 'Training accuracy:', model.score(np.array(X_train_full),np.array(y_train_full))\n",
    "print 'Test accuracy:', model.score(np.array(X_test_full), np.array(y_test_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like this 81% is about the best accuracy I can achieve using logistic regression, so I'll focus on my other models instead, since they are both getting 83+% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bestlogisticregression.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "model = LogisticRegression(penalty='l1', C=0.001)\n",
    "model.fit(X, y)\n",
    "\n",
    "joblib.dump(model, 'bestlogisticregression.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling in theory is not good for logistic regression, let's test that theory on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.580691299166\n",
      "Test accuracy: 0.568409343715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "model = LogisticRegression(penalty='l1', C=0.001)\n",
    "sc = StandardScaler()\n",
    "X = fullset.drop(['Label'], 1)\n",
    "y = fullset['Label']\n",
    "sc.fit(X)\n",
    "X = sc.transform(X)\n",
    "X_train_sc, X_test_sc, y_train_full, y_test_full = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model.fit(X_train_sc, y_train_full)\n",
    "\n",
    "print 'Training accuracy:', model.score(np.array(X_train_sc),np.array(y_train_full))\n",
    "print 'Test accuracy:', model.score(np.array(X_test_sc), np.array(y_test_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about using PCA on feature space and comparing performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.808462455304\n",
      "Test accuracy: 0.790322580645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "model = LogisticRegression(penalty='l1', C=0.001)\n",
    "pca = PCA(n_components=500)\n",
    "X = fullset.drop(['Label'], 1)\n",
    "y = fullset['Label']\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "X_train_pca, X_test_pca, y_train_full, y_test_full = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model.fit(X_train_pca, y_train_full)\n",
    "\n",
    "print 'Training accuracy:', model.score(np.array(X_train_pca),np.array(y_train_full))\n",
    "print 'Test accuracy:', model.score(np.array(X_test_pca), np.array(y_test_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there's a drop in performance. Since we're optimizing for accuracy, I'm going to just use the entire dataset. Might be worth investigating for speed reasons if trying to do real-time classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
